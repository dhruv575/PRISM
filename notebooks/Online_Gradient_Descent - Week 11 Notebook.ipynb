{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqvfHvPvNulb"
      },
      "source": [
        "# Online Gradient Descent Based Portfolio Optimizer\n",
        "\n",
        "\n",
        "This notebook extends upon Week 9's notebook by incorporating some ideas we talked about during the latest check in, such as running gradient descent over hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yVhpmX20WJ6R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM9-4Cw1WMPH",
        "outputId": "3d1030c2-409c-4c54-8957-04ac8f47694b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training period: 2022-01-01 to 2023-12-31\n",
            "Testing period: 2024-01-01 to 2025-04-08\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[***************       31%                       ]  34 of 109 completedFailed to get ticker 'GME' reason: Expecting value: line 1 column 1 (char 0)\n",
            "[**********************78%************           ]  85 of 109 completedFailed to get ticker 'RIVN' reason: Expecting value: line 1 column 1 (char 0)\n",
            "[**********************80%*************          ]  87 of 109 completedFailed to get ticker 'ENB' reason: Expecting value: line 1 column 1 (char 0)\n",
            "[**********************83%***************        ]  90 of 109 completedFailed to get ticker 'NIO' reason: Expecting value: line 1 column 1 (char 0)\n",
            "[**********************89%******************     ]  97 of 109 completedFailed to get ticker 'SPOT' reason: Expecting value: line 1 column 1 (char 0)\n",
            "[**********************90%******************     ]  98 of 109 completedFailed to get ticker 'TSM' reason: Expecting value: line 1 column 1 (char 0)\n",
            "[*********************100%***********************]  109 of 109 completed\n",
            "\n",
            "109 Failed downloads:\n",
            "['COST', 'AXP', 'PG', 'F', 'AVGO', 'PEP', 'MSFT', 'FMC', 'LRCX', 'VICI', 'AAPL', 'PARA', 'UNP', 'KO', 'INTC', 'MRNA', 'AMT', 'AMC', 'MOS', 'CVX', 'GM', 'TGT', 'UPS', 'TTWO', 'PSA', 'MA', 'NOC', 'SBUX', 'SPG', 'NFLX', 'MU', 'VRTX', 'NVDA', 'WY', 'ASML', 'GS', 'AMD', 'DAL', 'DE', 'MDLZ', 'FDX', 'LMT', 'CVS', 'DIS', 'ADI', 'BA', 'TSLA', 'NEE', 'MS', 'BAC', 'DUK', 'CAT', 'EA', 'MCD', 'SCHW', 'DLR', 'C', 'ORCL', 'META', 'WMT', 'SYY', 'GOOGL', 'JNJ', 'EMR', 'AMZN', 'RTX', 'HON', 'ISRG', 'YUM', 'PLD', 'GE', 'GILD', 'NXPI', 'TSN', 'LLY', 'EQIX', 'XOM', 'CF', 'CMCSA', 'PFE', 'IBM', 'QCOM', 'HRL', 'LYV', 'CSCO', 'UNH', 'CSX', 'BG', 'EOG', 'WBD', 'JPM', 'TXN', 'PSX', 'ADM', 'BMY', 'SLB', 'CAG', 'V', 'NKE', 'EQR']: JSONDecodeError('Expecting value: line 1 column 1 (char 0)')\n",
            "['O']: JS%ticker%NDecodeError('Expecting value: line 1 column 1 (char 0)')\n",
            "['GME', 'RIVN', 'ENB', 'NIO', 'SPOT', 'TSM']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n",
            "['D']: JSON%ticker%ecodeError('Expecting value: line 1 column 1 (char 0)')\n",
            "['SO']: J%ticker%NDecodeError('Expecting value: line 1 column 1 (char 0)')\n",
            "[**************        29%                       ]  32 of 109 completedFailed to get ticker 'GME' reason: Expecting value: line 1 column 1 (char 0)\n",
            "[**********************74%***********            ]  81 of 109 completedFailed to get ticker 'RIVN' reason: Expecting value: line 1 column 1 (char 0)\n",
            "[**********************75%***********            ]  82 of 109 completedFailed to get ticker 'ENB' reason: Expecting value: line 1 column 1 (char 0)\n",
            "[**********************83%***************        ]  90 of 109 completedFailed to get ticker 'NIO' reason: Expecting value: line 1 column 1 (char 0)\n",
            "[**********************86%****************       ]  94 of 109 completedFailed to get ticker 'SPOT' reason: Expecting value: line 1 column 1 (char 0)\n",
            "[**********************95%*********************  ]  104 of 109 completedFailed to get ticker 'TSM' reason: Expecting value: line 1 column 1 (char 0)\n",
            "[*********************100%***********************]  109 of 109 completed\n",
            "\n",
            "109 Failed downloads:\n",
            "['COST', 'VICI', 'F', 'PG', 'AXP', 'FMC', 'AVGO', 'MSFT', 'PEP', 'LRCX', 'CVX', 'KO', 'PARA', 'AMC', 'AAPL', 'UNP', 'AMT', 'MOS', 'INTC', 'MRNA', 'NOC', 'MA', 'TGT', 'NFLX', 'PSA', 'GM', 'SBUX', 'TTWO', 'SPG', 'UPS', 'DAL', 'MU', 'AMD', 'WY', 'NVDA', 'ASML', 'GS', 'VRTX', 'BA', 'MDLZ', 'TSLA', 'DE', 'CVS', 'FDX', 'LMT', 'DIS', 'ADI', 'EA', 'NEE', 'DUK', 'MS', 'DLR', 'MCD', 'BAC', 'CAT', 'SCHW', 'EMR', 'ORCL', 'JNJ', 'C', 'RTX', 'GOOGL', 'AMZN', 'META', 'SYY', 'WMT', 'GILD', 'NXPI', 'HON', 'YUM', 'LLY', 'ISRG', 'PLD', 'GE', 'EQIX', 'TSN', 'CMCSA', 'CF', 'LYV', 'IBM', 'PFE', 'HRL', 'XOM', 'QCOM', 'BG', 'EOG', 'CSX', 'TXN', 'WBD', 'CSCO', 'JPM', 'UNH', 'NKE', 'CAG', 'ADM', 'BMY', 'PSX', 'EQR', 'V', 'SLB']: JSONDecodeError('Expecting value: line 1 column 1 (char 0)')\n",
            "['GME', 'RIVN', 'ENB', 'NIO', 'SPOT', 'TSM']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n",
            "['O']: JS%ticker%NDecodeError('Expecting value: line 1 column 1 (char 0)')\n",
            "['D']: JSON%ticker%ecodeError('Expecting value: line 1 column 1 (char 0)')\n",
            "['SO']: J%ticker%NDecodeError('Expecting value: line 1 column 1 (char 0)')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (0, 109)\n",
            "Testing data shape: (0, 109)\n",
            "Missing values in training data: 0\n",
            "Missing values in testing data: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/sp/cw_2m19j25xbvgpdjz48gclh0000gn/T/ipykernel_85391/1548016008.py:63: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  train_prices = train_prices.fillna(method='ffill').fillna(method='bfill')\n",
            "/var/folders/sp/cw_2m19j25xbvgpdjz48gclh0000gn/T/ipykernel_85391/1548016008.py:64: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  test_prices = test_prices.fillna(method='ffill').fillna(method='bfill')\n"
          ]
        }
      ],
      "source": [
        "# Define the Magnificent 7 tickers\n",
        "tickers = [\n",
        "    # Technology & AI\n",
        "    \"AAPL\", \"MSFT\", \"NVDA\", \"GOOGL\", \"AMZN\", \"META\", \"AVGO\", \"ORCL\", \"IBM\", \"CSCO\",\n",
        "\n",
        "    # Financials & Banking\n",
        "    \"JPM\", \"V\", \"MA\", \"GS\", \"MS\", \"BAC\", \"C\", \"AXP\", \"SCHW\",\n",
        "\n",
        "    # Healthcare & Pharma\n",
        "    \"UNH\", \"JNJ\", \"LLY\", \"PFE\", \"MRNA\", \"BMY\", \"GILD\", \"CVS\", \"VRTX\", \"ISRG\",\n",
        "\n",
        "    # Consumer Goods & Retail\n",
        "    \"WMT\", \"NKE\", \"PG\", \"TGT\", \"COST\", \"KO\", \"PEP\", \"MCD\", \"SBUX\", \"YUM\",\n",
        "\n",
        "    # Energy & Utilities\n",
        "    \"XOM\", \"CVX\", \"NEE\", \"DUK\", \"SO\", \"D\", \"ENB\", \"SLB\", \"EOG\", \"PSX\",\n",
        "\n",
        "    # Industrials & Defense\n",
        "    \"DE\", \"LMT\", \"RTX\", \"BA\", \"CAT\", \"GE\", \"HON\", \"UPS\", \"EMR\", \"NOC\",\n",
        "\n",
        "    # Real Estate & Infrastructure\n",
        "    \"PLD\", \"AMT\", \"EQIX\", \"O\", \"SPG\", \"VICI\", \"DLR\", \"WY\", \"EQR\", \"PSA\",\n",
        "\n",
        "    # Transportation & Automotive\n",
        "    \"TSLA\", \"FDX\", \"UPS\", \"GM\", \"F\", \"RIVN\", \"NIO\", \"CSX\", \"UNP\", \"DAL\",\n",
        "\n",
        "    # Semiconductors\n",
        "    \"TSM\", \"ASML\", \"AMD\", \"TXN\", \"INTC\", \"MU\", \"QCOM\", \"LRCX\", \"NXPI\", \"ADI\",\n",
        "\n",
        "    # Agriculture & Food\n",
        "    \"ADM\", \"BG\", \"CF\", \"TSN\", \"MOS\", \"FMC\", \"CAG\", \"SYY\", \"HRL\", \"MDLZ\",\n",
        "\n",
        "    # Entertainment & Media\n",
        "    \"NFLX\", \"DIS\", \"PARA\", \"WBD\", \"CMCSA\", \"SPOT\", \"LYV\", \"TTWO\", \"EA\",\n",
        "\n",
        "    # Meme Stocks\n",
        "    \"GME\", \"AMC\"\n",
        "]\n",
        "\n",
        "# Fetch historical price data for 2022-2023 (training period)\n",
        "train_start_date = \"2022-01-01\"\n",
        "train_end_date = \"2023-12-31\"\n",
        "\n",
        "# Fetch historical price data for 2024 (testing period)\n",
        "test_start_date = \"2024-01-01\"\n",
        "test_end_date = datetime.now().strftime(\"%Y-%m-%d\")  # Today's date\n",
        "\n",
        "print(f\"Training period: {train_start_date} to {train_end_date}\")\n",
        "print(f\"Testing period: {test_start_date} to {test_end_date}\")\n",
        "\n",
        "# Download adjusted close prices for both periods\n",
        "train_prices = yf.download(tickers, start=train_start_date, end=train_end_date)[\"Close\"]\n",
        "test_prices = yf.download(tickers, start=test_start_date, end=test_end_date)[\"Close\"]\n",
        "\n",
        "print(f\"Training data shape: {train_prices.shape}\")\n",
        "print(f\"Testing data shape: {test_prices.shape}\")\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"Missing values in training data: {train_prices.isna().sum().sum()}\")\n",
        "print(f\"Missing values in testing data: {test_prices.isna().sum().sum()}\")\n",
        "\n",
        "# Fill any missing values with forward fill, then backward fill\n",
        "train_prices = train_prices.fillna(method='ffill').fillna(method='bfill')\n",
        "test_prices = test_prices.fillna(method='ffill').fillna(method='bfill')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fWqapwcbWSxL"
      },
      "outputs": [],
      "source": [
        "# Import the OnlinePortfolioOptimizer class from the previous code\n",
        "class OnlinePortfolioOptimizer:\n",
        "    def __init__(self,\n",
        "                 n_assets,\n",
        "                 initial_learning_rate=0.01,\n",
        "                 decay_rate=0.995,\n",
        "                 momentum=0.9,\n",
        "                 window_size=252,\n",
        "                 alphas=(1.0, 1.0, 1.0, 0.5, 0.2),\n",
        "                 transaction_penalty_factor=0.1,\n",
        "                 sparsity_factor=0.05):\n",
        "\n",
        "        self.n_assets = n_assets\n",
        "        self.initial_lr = initial_learning_rate\n",
        "        self.current_lr = initial_learning_rate\n",
        "        self.decay_rate = decay_rate\n",
        "        self.momentum = momentum\n",
        "        self.window_size = int(window_size) if window_size is not None else 252\n",
        "        self.alphas = alphas\n",
        "        self.transaction_penalty_factor = transaction_penalty_factor\n",
        "        self.sparsity_factor = sparsity_factor\n",
        "\n",
        "        # Store previous weights for transaction cost calculation\n",
        "        self.previous_weights = None\n",
        "\n",
        "        # Initialize weights equally\n",
        "        self.weights = torch.ones(n_assets) / n_assets\n",
        "        self.velocity = torch.zeros(n_assets)\n",
        "        self.t = 0\n",
        "\n",
        "    def _decay_learning_rate(self):\n",
        "        self.current_lr = self.initial_lr * (self.decay_rate ** self.t)\n",
        "\n",
        "    def _calculate_expected_returns_gradient(self, prices_df):\n",
        "        window_data = prices_df.iloc[-self.window_size:]\n",
        "        returns = window_data.pct_change().dropna()\n",
        "        expected_returns = returns.mean()\n",
        "        grad = expected_returns / 0.5\n",
        "        return torch.tensor(grad.values, dtype=torch.float32)\n",
        "\n",
        "    def _calculate_sortino_gradient(self, prices_df):\n",
        "        window_data = prices_df.iloc[-self.window_size:]\n",
        "        returns = window_data.pct_change().dropna()\n",
        "        portfolio_returns = torch.matmul(torch.tensor(returns.values, dtype=torch.float32), self.weights)\n",
        "\n",
        "        rf_daily = 0.02 / 252\n",
        "        excess_returns = torch.tensor(returns.values, dtype=torch.float32) - rf_daily\n",
        "\n",
        "        # Find indices where portfolio returns are negative\n",
        "        negative_mask = portfolio_returns < 0\n",
        "        negative_returns = excess_returns[negative_mask]\n",
        "\n",
        "        if len(negative_returns) > 0:\n",
        "            downside_std = torch.sqrt(torch.mean(torch.square(negative_returns)))\n",
        "        else:\n",
        "            downside_std = torch.tensor(1e-6)\n",
        "\n",
        "        grad = (torch.mean(excess_returns, dim=0) / (3.0 * downside_std))\n",
        "        return grad\n",
        "\n",
        "    def _calculate_drawdown_gradient(self, prices_df):\n",
        "        window_data = prices_df.iloc[-self.window_size:]\n",
        "        returns = window_data.pct_change().dropna()\n",
        "\n",
        "        # Convert returns to torch tensor\n",
        "        returns_tensor = torch.tensor(returns.values, dtype=torch.float32)\n",
        "\n",
        "        # Calculate cumulative returns for the portfolio\n",
        "        portfolio_returns = torch.matmul(returns_tensor, self.weights)\n",
        "        cumulative_returns = torch.cumprod(1 + portfolio_returns, dim=0)\n",
        "\n",
        "        # Find the maximum drawdown point\n",
        "        peak = torch.cummax(cumulative_returns, dim=0)[0]\n",
        "        drawdown = cumulative_returns / peak - 1\n",
        "        max_dd_idx = torch.argmin(drawdown)\n",
        "\n",
        "        # Calculate gradient\n",
        "        if max_dd_idx > 0:\n",
        "            grad = -torch.mean(returns_tensor[:max_dd_idx+1], dim=0)\n",
        "        else:\n",
        "            grad = torch.zeros(self.n_assets)\n",
        "\n",
        "        return grad\n",
        "\n",
        "    def _project_onto_simplex(self, weights):\n",
        "        if torch.sum(weights) <= 0:\n",
        "            return torch.ones(self.n_assets) / self.n_assets\n",
        "\n",
        "        sorted_weights, _ = torch.sort(weights, descending=True)\n",
        "        cumulative_sum = torch.cumsum(sorted_weights, dim=0)\n",
        "\n",
        "        k = 1\n",
        "        for i in range(1, len(weights) + 1):\n",
        "            v_k = (cumulative_sum[i-1] - 1) / i\n",
        "            if i == len(weights) or sorted_weights[i] <= v_k:\n",
        "                k = i\n",
        "                break\n",
        "\n",
        "        threshold = v_k\n",
        "        return torch.maximum(weights - threshold, torch.zeros_like(weights))\n",
        "\n",
        "    def step(self, prices_df, verbose=True):\n",
        "        if len(prices_df) < self.window_size:\n",
        "            if verbose:\n",
        "                print(f\"Insufficient data points ({len(prices_df)}/{self.window_size}). Returning current weights.\")\n",
        "            return self.weights\n",
        "\n",
        "        self.t += 1\n",
        "        prev_lr = self.current_lr\n",
        "        self._decay_learning_rate()\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n===== Optimization Step {self.t} =====\")\n",
        "            print(f\"Learning rate: {prev_lr:.6f} → {self.current_lr:.6f}\")\n",
        "\n",
        "            if self.previous_weights is not None:\n",
        "                # Calculate portfolio sparsity metrics\n",
        "                nonzero_positions = (self.weights > 0.001).sum().item()\n",
        "                l1_norm = torch.sum(torch.abs(self.weights)).item()\n",
        "                print(f\"Current portfolio stats:\")\n",
        "                print(f\"  Active positions (>0.1%): {nonzero_positions} / {self.n_assets}\")\n",
        "                print(f\"  L1 norm: {l1_norm:.4f}\")\n",
        "\n",
        "            # Log current portfolio stats\n",
        "            weights_list = self.weights.cpu().tolist()\n",
        "            top_holdings = [(idx, w) for idx, w in sorted(\n",
        "                enumerate(weights_list), key=lambda x: x[1], reverse=True) if w > 0.01]\n",
        "\n",
        "            if top_holdings:\n",
        "                print(\"Current top holdings (>1%):\")\n",
        "                for idx, weight in top_holdings[:5]:  # Show top 5\n",
        "                    ticker = prices_df.columns[idx]\n",
        "                    print(f\"  {ticker}: {weight*100:.2f}%\")\n",
        "\n",
        "                print(f\"Portfolio concentration (HHI): {(self.weights**2).sum().item()*10000:.2f}\")\n",
        "\n",
        "        try:\n",
        "            # Calculate gradients\n",
        "            er_grad = self._calculate_expected_returns_gradient(prices_df)\n",
        "            sortino_grad = self._calculate_sortino_gradient(prices_df)\n",
        "            dd_grad = self._calculate_drawdown_gradient(prices_df)\n",
        "\n",
        "            if verbose:\n",
        "                # Log gradient magnitudes for each component\n",
        "                er_norm = torch.norm(er_grad).item()\n",
        "                sortino_norm = torch.norm(sortino_grad).item()\n",
        "                dd_norm = torch.norm(dd_grad).item()\n",
        "\n",
        "                print(\"\\nGradient components:\")\n",
        "                print(f\"  Expected returns gradient magnitude: {er_norm:.4f}\")\n",
        "                print(f\"  Sortino ratio gradient magnitude: {sortino_norm:.4f}\")\n",
        "                print(f\"  Drawdown gradient magnitude: {dd_norm:.4f}\")\n",
        "\n",
        "                # Identify stocks with the largest gradients\n",
        "                for grad_name, grad in [(\"Expected returns\", er_grad),\n",
        "                                       (\"Sortino\", sortino_grad),\n",
        "                                       (\"Drawdown\", dd_grad)]:\n",
        "\n",
        "                    top_idx = torch.argsort(grad, descending=True)[:3]\n",
        "                    print(f\"\\n  Top 3 positive {grad_name} signals:\")\n",
        "                    for idx in top_idx:\n",
        "                        # Convert PyTorch tensor index to Python int for DataFrame indexing\n",
        "                        idx_int = idx.item()\n",
        "                        ticker = prices_df.columns[idx_int]\n",
        "                        print(f\"    {ticker}: {grad[idx].item():.4f}\")\n",
        "\n",
        "            # Store current weights for transaction penalty calculation\n",
        "            old_weights = self.weights.clone()\n",
        "\n",
        "            # Calculate transaction cost penalty gradient (if we have previous weights)\n",
        "            if self.previous_weights is not None:\n",
        "                transaction_penalty_grad = -torch.sign(old_weights - self.previous_weights) * self.transaction_penalty_factor\n",
        "\n",
        "                if verbose:\n",
        "                    penalty_magnitude = torch.norm(transaction_penalty_grad).item()\n",
        "                    print(f\"\\nTransaction penalty gradient magnitude: {penalty_magnitude:.4f}\")\n",
        "            else:\n",
        "                transaction_penalty_grad = torch.zeros_like(old_weights)\n",
        "                if verbose:\n",
        "                    print(\"\\nNo transaction penalty for first step (no previous weights)\")\n",
        "\n",
        "            # Calculate L1 sparsity gradient\n",
        "            l1_sparsity_grad = -torch.sign(old_weights) * self.sparsity_factor\n",
        "\n",
        "            if verbose:\n",
        "                sparsity_magnitude = torch.norm(l1_sparsity_grad).item()\n",
        "                print(f\"L1 sparsity gradient magnitude: {sparsity_magnitude:.4f}\")\n",
        "\n",
        "            # Combine all gradients using alpha weights\n",
        "            total_grad = (self.alphas[0] * er_grad +\n",
        "                         self.alphas[1] * sortino_grad -\n",
        "                         self.alphas[2] * dd_grad +\n",
        "                         self.alphas[3] * transaction_penalty_grad +\n",
        "                         self.alphas[4] * l1_sparsity_grad)\n",
        "\n",
        "            # Update velocity and weights\n",
        "            old_velocity = self.velocity.clone()\n",
        "            self.velocity = self.momentum * old_velocity + self.current_lr * total_grad\n",
        "\n",
        "            if verbose:\n",
        "                print(\"\\nMomentum update:\")\n",
        "                print(f\"  Momentum factor: {self.momentum:.4f}\")\n",
        "                print(f\"  Velocity change magnitude: {torch.norm(self.velocity - old_velocity).item():.4f}\")\n",
        "\n",
        "            new_weights = old_weights + self.velocity\n",
        "\n",
        "            # Calculate transaction costs for reporting only (no hard constraint anymore)\n",
        "            costs = torch.sum(torch.abs(new_weights - old_weights)) * 0.001\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"\\nTransaction costs: {costs*100:.4f}%\")\n",
        "\n",
        "            # Store current weights as previous for next iteration\n",
        "            self.previous_weights = old_weights.clone()\n",
        "\n",
        "            # Project onto simplex\n",
        "            self.weights = self._project_onto_simplex(new_weights)\n",
        "\n",
        "            if verbose:\n",
        "                # Log biggest weight changes\n",
        "                weight_changes = self.weights - old_weights\n",
        "                increase_idx = torch.argsort(weight_changes, descending=True)[:3]\n",
        "                decrease_idx = torch.argsort(weight_changes)[:3]\n",
        "\n",
        "                print(\"\\nBiggest position increases:\")\n",
        "                for idx in increase_idx:\n",
        "                    # Convert PyTorch tensor index to Python int\n",
        "                    idx_int = idx.item()\n",
        "                    ticker = prices_df.columns[idx_int]\n",
        "                    old_pct = old_weights[idx].item() * 100\n",
        "                    new_pct = self.weights[idx].item() * 100\n",
        "                    print(f\"  {ticker}: {old_pct:.2f}% → {new_pct:.2f}% (Δ{new_pct-old_pct:+.2f}%)\")\n",
        "\n",
        "                print(\"\\nBiggest position decreases:\")\n",
        "                for idx in decrease_idx:\n",
        "                    # Convert PyTorch tensor index to Python int\n",
        "                    idx_int = idx.item()\n",
        "                    ticker = prices_df.columns[idx_int]\n",
        "                    old_pct = old_weights[idx].item() * 100\n",
        "                    new_pct = self.weights[idx].item() * 100\n",
        "                    print(f\"  {ticker}: {old_pct:.2f}% → {new_pct:.2f}% (Δ{new_pct-old_pct:+.2f}%)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in step: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            # Return current weights if there's an error\n",
        "\n",
        "        return self.weights\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EfKHIU9xWgz8"
      },
      "outputs": [],
      "source": [
        "def evaluate_hyperparams(prices_df, hyperparams, steps=30):\n",
        "    optimizer = OnlinePortfolioOptimizer(\n",
        "        n_assets=prices_df.shape[1],\n",
        "        initial_learning_rate=hyperparams[\"initial_learning_rate\"],\n",
        "        decay_rate=hyperparams[\"decay_rate\"],\n",
        "        momentum=hyperparams[\"momentum\"],\n",
        "        window_size=hyperparams[\"window_size\"],\n",
        "    )\n",
        "    weights = None\n",
        "    for i in range(steps):\n",
        "        sub_df = prices_df.iloc[:optimizer.window_size + i]\n",
        "        weights = optimizer.step(sub_df, verbose=False)\n",
        "\n",
        "    # Evaluate using cumulative return\n",
        "    returns = prices_df.pct_change().dropna()\n",
        "    portfolio_returns = (returns @ weights.numpy()).cumsum()\n",
        "    return portfolio_returns.iloc[-1]\n",
        "\n",
        "def evaluate_objective(prices_df, hyperparams, alphas=(1.0, 1.0, 1.0, 0.5, 0.2), steps=30):\n",
        "    optimizer = OnlinePortfolioOptimizer(\n",
        "        n_assets=prices_df.shape[1],\n",
        "        initial_learning_rate=hyperparams[\"initial_learning_rate\"],\n",
        "        decay_rate=hyperparams[\"decay_rate\"],\n",
        "        momentum=hyperparams[\"momentum\"],\n",
        "        window_size=int(hyperparams[\"window_size\"]),\n",
        "        alphas=alphas,\n",
        "        transaction_penalty_factor=0.1,\n",
        "        sparsity_factor=0.05,\n",
        "    )\n",
        "\n",
        "    prev_weights = None\n",
        "    total_score = 0\n",
        "\n",
        "    for i in range(steps):\n",
        "        sub_df = prices_df.iloc[:optimizer.window_size + i]\n",
        "        weights = optimizer.step(sub_df, verbose=False)\n",
        "\n",
        "        # --- Compute each component manually ---\n",
        "        window_returns = sub_df.pct_change().dropna()\n",
        "        portfolio_returns = window_returns @ weights.numpy()\n",
        "        excess_returns = portfolio_returns - (0.02 / 252)\n",
        "\n",
        "        # Expected Return\n",
        "        er = np.mean(excess_returns)\n",
        "\n",
        "        # Sortino Ratio\n",
        "        downside = excess_returns[excess_returns < 0]\n",
        "        sortino = er / (np.std(downside) + 1e-6)\n",
        "\n",
        "        # Max Drawdown\n",
        "        cumulative = (1 + portfolio_returns).cumprod()\n",
        "        running_max = np.maximum.accumulate(cumulative)\n",
        "        drawdown = cumulative / running_max - 1\n",
        "        max_dd = np.min(drawdown)\n",
        "\n",
        "        # Transaction Cost Penalty\n",
        "        tc_penalty = 0\n",
        "        if prev_weights is not None:\n",
        "            tc_penalty = np.sum(np.abs(weights.numpy() - prev_weights.numpy())) * 0.001\n",
        "\n",
        "        # Sparsity Penalty\n",
        "        sparsity_penalty = np.sum(np.abs(weights.numpy()))\n",
        "\n",
        "        prev_weights = weights.clone()\n",
        "\n",
        "        # Combine using alphas\n",
        "        score = (\n",
        "            alphas[0] * er +\n",
        "            alphas[1] * sortino -\n",
        "            alphas[2] * abs(max_dd) -\n",
        "            alphas[3] * tc_penalty -\n",
        "            alphas[4] * sparsity_penalty\n",
        "        )\n",
        "        total_score += score\n",
        "\n",
        "    return total_score / steps\n",
        "\n",
        "\n",
        "def hyperparam_gradient_descent(prices_df, initial_params, lr=0.01, n_iter=10):\n",
        "    params = initial_params.copy()\n",
        "    keys = list(params.keys())\n",
        "    history = []\n",
        "\n",
        "    for iteration in range(n_iter):\n",
        "        grads = {}\n",
        "        # base_score = evaluate_hyperparams(prices_df, params)\n",
        "        base_score = evaluate_objective(prices_df, params)\n",
        "\n",
        "        # Log the current state\n",
        "        record = {\"iteration\": iteration + 1, \"score\": base_score}\n",
        "        record.update(params.copy())\n",
        "        history.append(record)\n",
        "\n",
        "        for key in keys:\n",
        "            eps = 1e-3\n",
        "            perturbed = params.copy()\n",
        "            perturbed[key] += eps\n",
        "\n",
        "            # Clip\n",
        "            if \"momentum\" in key or \"decay\" in key:\n",
        "                perturbed[key] = min(max(perturbed[key], 0), 1)\n",
        "            if \"window_size\" in key:\n",
        "                perturbed[key] = max(30, int(perturbed[key]))\n",
        "\n",
        "            # score = evaluate_hyperparams(prices_df, perturbed)\n",
        "            score = evaluate_objective(prices_df, perturbed)\n",
        "            grads[key] = (score - base_score) / eps\n",
        "\n",
        "        for key in keys:\n",
        "            params[key] += lr * grads[key]\n",
        "\n",
        "    return params, history\n",
        "\n",
        "def plot_hyperparam_optimization(history):\n",
        "    \"\"\"\n",
        "    history: List of dicts, where each dict contains:\n",
        "      - 'iteration': int\n",
        "      - 'score': float\n",
        "      - each hyperparam: float\n",
        "    \"\"\"\n",
        "    if not history:\n",
        "        print(\"No optimization history to plot.\")\n",
        "        return\n",
        "\n",
        "    keys = [k for k in history[0].keys() if k != 'iteration' and k != 'score']\n",
        "    iterations = [h['iteration'] for h in history]\n",
        "    scores = [h['score'] for h in history]\n",
        "\n",
        "    fig, axs = plt.subplots(len(keys) + 1, 1, figsize=(8, 4 * (len(keys) + 1)))\n",
        "\n",
        "    # Plot score\n",
        "    axs[0].plot(iterations, scores, marker='o')\n",
        "    axs[0].set_title(\"Performance Metric Over Iterations\")\n",
        "    axs[0].set_ylabel(\"Score (e.g., cumulative return)\")\n",
        "    axs[0].grid(True)\n",
        "\n",
        "    # Plot each hyperparameter\n",
        "    for i, key in enumerate(keys):\n",
        "        values = [h[key] for h in history]\n",
        "        axs[i+1].plot(iterations, values, marker='o')\n",
        "        axs[i+1].set_title(f\"{key} Over Iterations\")\n",
        "        axs[i+1].set_ylabel(key)\n",
        "        axs[i+1].grid(True)\n",
        "\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SkQlmek5Ew_5",
        "outputId": "4c0d33e0-8af9-425d-f006-de3872335a06"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "cannot convert float NaN to integer",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m initial_hyperparams \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_learning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.10\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecay_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.80\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmomentum\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m      6\u001b[0m }\n\u001b[0;32m----> 8\u001b[0m best_params, history \u001b[38;5;241m=\u001b[39m \u001b[43mhyperparam_gradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_prices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_hyperparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m plot_hyperparam_optimization(history)\n",
            "Cell \u001b[0;32mIn[6], line 87\u001b[0m, in \u001b[0;36mhyperparam_gradient_descent\u001b[0;34m(prices_df, initial_params, lr, n_iter)\u001b[0m\n\u001b[1;32m     85\u001b[0m grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# base_score = evaluate_hyperparams(prices_df, params)\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m base_score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_objective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprices_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Log the current state\u001b[39;00m\n\u001b[1;32m     90\u001b[0m record \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration\u001b[39m\u001b[38;5;124m\"\u001b[39m: iteration \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: base_score}\n",
            "Cell \u001b[0;32mIn[6], line 25\u001b[0m, in \u001b[0;36mevaluate_objective\u001b[0;34m(prices_df, hyperparams, alphas, steps)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_objective\u001b[39m(prices_df, hyperparams, alphas\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.2\u001b[39m), steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m):\n\u001b[1;32m     20\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m OnlinePortfolioOptimizer(\n\u001b[1;32m     21\u001b[0m         n_assets\u001b[38;5;241m=\u001b[39mprices_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     22\u001b[0m         initial_learning_rate\u001b[38;5;241m=\u001b[39mhyperparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_learning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     23\u001b[0m         decay_rate\u001b[38;5;241m=\u001b[39mhyperparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecay_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     24\u001b[0m         momentum\u001b[38;5;241m=\u001b[39mhyperparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmomentum\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m---> 25\u001b[0m         window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwindow_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     26\u001b[0m         alphas\u001b[38;5;241m=\u001b[39malphas,\n\u001b[1;32m     27\u001b[0m         transaction_penalty_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m     28\u001b[0m         sparsity_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m,\n\u001b[1;32m     29\u001b[0m     )\n\u001b[1;32m     31\u001b[0m     prev_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     total_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
          ]
        }
      ],
      "source": [
        "initial_hyperparams = {\n",
        "    \"initial_learning_rate\": 0.10,\n",
        "    \"decay_rate\": 0.80,\n",
        "    \"momentum\": 0.5,\n",
        "    \"window_size\": 30,\n",
        "}\n",
        "\n",
        "best_params, history = hyperparam_gradient_descent(train_prices, initial_hyperparams, lr=0.1, n_iter=5)\n",
        "plot_hyperparam_optimization(history)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
